{"name":"Hncrawl","tagline":"A scrapy-based Hacker News crawler.","body":"HNCrawl\r\n=======\r\n\r\n*A [`scrapy`][scrapy]-based [Hacker News][hn] crawler.*\r\n\r\n**Introduction**\r\n\r\nHNCrawl is a tiny, simple [`scrapy`][scrapy]-based crawler which grabs the html \r\ncontent of pages linked to the front page of hacker news.\r\n\r\n## Examples\r\n\r\n### Installation\r\n\r\n    $ pip install scrapy\r\n    $ git clone git@github.com:mvanveen/hncrawl.git\r\n    \r\n### Scraping\r\n\r\n**Note**: Please be sure to keep in mind that the [`Crawl-Delay`][crawl] value set in the HN [`robots.txt`][robots] file is set to **30 seconds**.  **Please be sure to avoid using the scraper more than once per 30 seconds!**\r\n    \r\n**Scrape the links from the front page of HN**    \r\n    \r\n    $ scrapy crawl hnspider\r\n\r\n**Scrape items and return json summary of items scraped into `items.json`**\r\n\r\n    $ scrapy crawl alias_scrape -o items.json -t json\r\n    \r\n## Output\r\n\r\nHere is an example file hierarchy.  Folders are a hex digest\r\nof the SHA1 hash of the hacker news item url.\r\n\r\n\r\n     ├── out\r\n     │   ├── 000f86c7547b47a700dee0879a0fe08b4597360f\r\n     │   │   └── index.html\r\n     │   ├── 0190cbad182ab3bc9a92482d169f38e363ca3c57\r\n     │   │   └── index.html\r\n     │   ├── 02bae9642c8dd4b75a593c1c42beff62824ee8fc\r\n     │   │   └── index.html\r\n     │   ├── 05c1460571f0ac45f77bf2ecbd3cba8b85c20621\r\n     │   │   └── index.html\r\n     │   ├── 0b1587a3dbe9996d10a0fd3250f75462ebd59a0b\r\n     │   │   └── index.html\r\n     │   ├── 0c5c67585004e03341e6a87d2db5257b93337b86\r\n     │   │   └── \r\n\r\nThe JSON summary of news items look like this:\r\n\r\n\t{'title': u'EFF Wins Protection for Time Zone Database',\r\n\t 'url': u'https://www.eff.org/press/releases/eff-wins-protection-time-zone-database'}\r\n\r\n## Dependencies\r\n\r\n- [scrapy][scrapy]\r\n- [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/)\r\n\r\n## License\r\n\r\nHNCrawl is MIT licensed.\r\n\r\n[crawl]: http://en.wikipedia.org/wiki/Robots_exclusion_standard#Crawl-delay_directive\r\n[hn]: http://news.ycombinator.com\r\n[robots]: http://news.ycombinator.com/robots.txt\r\n[scrapy]: http://scrapy.org/\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}